{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "# Load \n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import spacy \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "#data = pd.read_excel('D://thesis/all_YouScan_Mentions_3_new.xlsx'\n",
    "#skip_cols = ['Дайджест тексту']\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "#print(data.head())\n",
    "\n",
    "# Descriptive statistics\n",
    "#print(data.describe())\n",
    "\n",
    "#data['id'] = data.index\n",
    "#data['Дайджест тексту'] = data['Дайджест тексту'].astype(str)\n",
    "#data.dropna(subset=['Дайджест тексту'], inplace=True)\n",
    "#data = data.drop(columns = ['URL', 'Профіль', 'Профіль місця публікації', 'Нотатки', 'Лайки', 'Love', 'Haha', 'Wow',\n",
    "#       'Sad', 'Angry', 'Dislikes', 'Коментарі', 'Репости', 'Рейтинг', 'URL зображення', 'Призначено', 'Опрацьоване'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting by place of post\n",
    "data.groupby('Джерело').count().sort_values('Unnamed: 0', ascending = False). head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data posts, reposts, comments counting\n",
    "data_posts = data[data['Тип посту'] == 'Пост']\n",
    "len(data_posts)\n",
    "\n",
    "data_repost = data[data['Тип посту'] == 'Репост']\n",
    "len(data_repost)\n",
    "\n",
    "data_comment = data[data['Тип посту'] == 'Коментар']\n",
    "len(data_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# russian posts merging\n",
    "#data_lang_1 = pd.read_excel('data_lang1.xlsx')\n",
    "#data_lang_2 = pd.read_excel('data_lang112.xlsx')\n",
    "#data_lang_3 = pd.read_excel('data_lang_11.xlsx')\n",
    "#data_lang_4 = pd.read_excel('data_lang_2.xlsx')\n",
    "#data_lang_5 = pd.read_excel('data_lang22.xlsx')\n",
    "#data_lang_6 = pd.read_excel('data_lang3.xlsx')\n",
    "#data_lang_7 = pd.read_excel('data_lang_4.xlsx')\n",
    "#data_lang_8 = pd.read_excel('data_lang5.xlsx')\n",
    "#data_lang_9 = pd.read_excel('data_lang55.xlsx')\n",
    "#data_lang_10 = pd.read_excel('data_lang555.xlsx')\n",
    "#data_lang_11 = pd.read_excel('data_lang_6.xlsx')\n",
    "#data_lang_12 = pd.read_excel('data_lang_7.xlsx')\n",
    "#data_lang_13 = pd.read_excel('data_lang_9.xlsx')\n",
    "#data_lang_14 = pd.read_excel('data_lang10.xlsx')\n",
    "#data_lang_15 = pd.read_excel('data_lang11.xlsx')\n",
    "#data_lang_16 = pd.read_excel('data_lang12.xlsx')\n",
    "#data_lang_17 = pd.read_excel('data_lang13.xlsx')\n",
    "\n",
    "#data_lang_1 = data_lang_1.rename(columns = {'id': ' id'})\n",
    "#data_lang_1 = data_lang_1.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_2 = data_lang_2.rename(columns = {'id': ' id'})\n",
    "#data_lang_2 = data_lang_2.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_3 = data_lang_3.rename(columns = {'id': ' id'})\n",
    "#data_lang_3 = data_lang_3.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_4 = data_lang_4.rename(columns = {'id': ' id'})\n",
    "#data_lang_4 = data_lang_4.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_5 = data_lang_5.rename(columns = {'id': ' id'})\n",
    "#data_lang_5 = data_lang_5.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_6 = data_lang_6.rename(columns = {'id': ' id'})\n",
    "#data_lang_6 = data_lang_6.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_7 = data_lang_7.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_8 = data_lang_8.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_9 = data_lang_9.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_10 = data_lang_10.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_11 = data_lang_11.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_12 = data_lang_12.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_13 = data_lang_13.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_14 = data_lang_14.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_15 = data_lang_15.rename(columns = {'id': ' id', 'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_2 = data_lang_2.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_16 = data_lang_16.rename(columns = {'id': ' id'})\n",
    "#data_lang_16 = data_lang_16.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "#data_lang_17 = data_lang_17.rename(columns = {'id': ' id'})\n",
    "#data_lang_17 = data_lang_17.rename(columns = {'Дайджест текста': ' Дайджест текста'})\n",
    "\n",
    "\n",
    "#data_lang = pd.concat([data_lang_1,\n",
    "#data_lang_2,\n",
    "#data_lang_3,\n",
    "#data_lang_4,\n",
    "#data_lang_5,\n",
    "#data_lang_6,\n",
    "#data_lang_7,\n",
    "#data_lang_8,\n",
    "#data_lang_9,\n",
    "#data_lang_10,\n",
    "#data_lang_11,\n",
    "#data_lang_12,\n",
    "#data_lang_13,\n",
    "#data_lang_14,\n",
    "#data_lang_15, \n",
    "#data_lang_16,\n",
    "#data_lang_17])\n",
    "\n",
    "#data_lang = data_lang.rename(columns = {' id': 'id'})\n",
    "\n",
    "#data_new = data.merge(data_lang, how = 'left', on = 'id')\n",
    "#data_new = data_new.drop_duplicates()\n",
    "#data_new[' Дайджест текста'] = data_new[' Дайджест текста'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# russian text reading\n",
    "#data = pd.read_excel('data_all_ru.xlsx', usecols=lambda x: x not in skip_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "custom_stopwords  = [\"интеллект\", \"искусственный\", \"ии\", \"ai\", \"intelligence\", \"artificial\"\n",
    "] # \"learning\", \"machine\"]\n",
    "\n",
    "# spaCy model for Russian\n",
    "nlp = spacy.load('ru_core_news_md')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Lemmatization with spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    # Removing punctuation and numbers and short words\n",
    "    tokens = [token for token in tokens if token.isalpha() and len(token) > 2]\n",
    "    # Removing stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [word for word in tokens if word not in custom_stopwords]\n",
    "\n",
    "    # Joining tokens back into text\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_advertisement_posts(data):\n",
    "\n",
    "    # Function to check if a post contains advertisement keywords\n",
    "    def has_ad_keywords(text):\n",
    "        ad_keywords = [\n",
    "            \"акция\", \"скидка\", \"распродажа\", \"предложение\", \"бесплатно\",\n",
    "        \"покупка\", \"продажа\", \"купить\", \"выгодно\", \"акционный\",\n",
    "        \"реклама\", \"оптовый\", \"магазин\", \"купон\",\n",
    "        \"покупатель\", \"уникальный\", \"специальный\", \"доставка\", \"эксклюзивный\", 'вакансия', \n",
    "        \"продвижение\", \"популярный\", \"доступный\",\n",
    "        \"бонус\", \"эксклюзив\", \"гарантированный\", \"лучший\", \"ограниченный\",\n",
    "        \"экономия\", \"покупательский\", \"коллекция\", \"сезонный\", \"хит\",\n",
    "        \"приобрести\", \"востребованный\", \"дешево\", \"инновационный\", \"экономичный\"\n",
    "        'зарабатывать', 'работа'\n",
    "        ]\n",
    "        count = sum(1 for keyword in ad_keywords if keyword in text.lower())\n",
    "        return count > 3\n",
    "\n",
    "\n",
    "    # Filter out advertisement posts\n",
    "    data_filtered = data[~data['cleaned_post'].astype(str).apply(has_ad_keywords)]\n",
    "    return data_filtered\n",
    "\n",
    "#data_filtered = remove_advertisement_posts(data)\n",
    "#data = data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news media \n",
    "data_smi = data[data['Тип джерела'] == 'ЗМІ']\n",
    "data_groups = data[data['Демографія'] == 'Спільнота']\n",
    "\n",
    "# social media\n",
    "data_social = data[data['Тип джерела'].isin(['Соціальні мережі', 'Форум', 'Месенджери', 'Блог'])]\n",
    "\n",
    "# remove duplicates \n",
    "data_social = data_social.drop_duplicates(subset=[' Дайджест текста'])\n",
    "data_smi = data_smi.drop_duplicates(subset=[' Дайджест текста'])\n",
    "data_groups = data_groups.drop_duplicates(subset=[' Дайджест текста'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing applied\n",
    "data_groups['cleaned_post'] = data_groups['Дайджест тексту'].apply(preprocess_text)\n",
    "data_groups.to_csv('data_groups_pro.csv')\n",
    "\n",
    "data_smi['cleaned_post'] = data_smi['Дайджест тексту'].apply(preprocess_text)\n",
    "data_smi.to_csv('data_smi_pro.csv')\n",
    "\n",
    "data_social['cleaned_post'] = data_social['Дайджест тексту'].apply(preprocess_text)\n",
    "data_social.to_csv('data_social_pro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
